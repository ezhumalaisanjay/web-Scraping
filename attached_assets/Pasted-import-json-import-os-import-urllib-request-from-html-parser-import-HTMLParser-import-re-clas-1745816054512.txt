import json
import os
import urllib.request
from html.parser import HTMLParser
import re

class MyHTMLParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.paragraphs = []
        self.contact_info = []
        self.services = []
        self.products = []
        self.company_name = None
        self.capture = False
        self.headings = []  # Store heading data to analyze better
        self.current_tag = None  # Track the current tag being processed

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag  # Track the current tag
        if tag == 'p' or tag == 'a':
            self.capture = True
        elif tag == 'h1' or tag == 'h2' or tag == 'h3':
            self.capture = True  # Capture text in headers (for potential company name)

    def handle_endtag(self, tag):
        if tag == 'p' or tag == 'a' or tag == 'h1' or tag == 'h2' or tag == 'h3':
            self.capture = False
        self.current_tag = None  # Reset current tag when ending a tag

    def handle_data(self, data):
        if self.capture:
            text = data.strip()
            if self.current_tag in ['h1', 'h2', 'h3']:
                self.headings.append(text)
            # Check for company name: First heading tag or the title of the page
            if not self.company_name and self.headings:
                self.company_name = self.headings[0]  # Select the first heading as the company name
            # Check for contact info (email, phone, address)
            elif re.search(r'[\w\.-]+@[\w\.-]+', text):
                self.contact_info.append(text)
            elif re.search(r'\+?\d{1,4}?[-.\s]?\(?\d{1,3}?\)?[-.\s]?\d{1,4}[-.\s]?\d{1,4}[-.\s]?\d{1,4}', text):
                self.contact_info.append(text)
            elif re.search(r'\d{1,4}[\w\s]+[,\-.\d]*\s*(Street|Ave|Road|Boulevard|Lane)', text):
                self.contact_info.append(text)
            elif 'service' in text.lower():
                self.services.append(text)
            elif 'product' in text.lower() or 'offer' in text.lower():
                self.products.append(text)
            else:
                self.paragraphs.append(text)

def lambda_handler(event, context):
    target_url = "https://www.contus.com/"

    if not target_url:
        return {
            'statusCode': 400,
            'body': json.dumps({'error': 'WEBSITE_URL environment variable not set.'})
        }

    scraped_data = scrape_data(target_url)

    if scraped_data is not None:
        summary = {
            'company_name': scraped_data.company_name,
            'contact_info': scraped_data.contact_info,
            'services': scraped_data.services,
            'products': scraped_data.products
        }
        return {
            'statusCode': 200,
            'body': json.dumps(summary)
        }
    else:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': 'Failed to scrape data.'})
        }

def scrape_data(url):
    try:
        headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/123.0.0.0 Safari/537.36'
            )
        }

        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=10) as response:
            html_content = response.read().decode('utf-8')

        parser = MyHTMLParser()
        parser.feed(html_content)

        return parser

    except Exception as e:
        print(f"An error occurred during scraping: {e}")
        return None
